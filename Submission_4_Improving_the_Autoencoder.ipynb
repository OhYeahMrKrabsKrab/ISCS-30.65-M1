{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI-bgL3RGvxO",
        "outputId": "28f1eebd-222b-4fa0-ac4d-81eaf46f07ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 5s 475ms/step - loss: -1616.8760 - val_loss: -1903.6166\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 3s 393ms/step - loss: -1902.7467 - val_loss: -1903.9375\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 4s 532ms/step - loss: -1902.7966 - val_loss: -1903.9437\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 3s 294ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 3s 358ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 3s 386ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 2s 286ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 2s 220ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 3s 323ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 2s 189ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 2s 193ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 2s 191ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 2s 254ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 3s 317ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 2s 191ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 2s 194ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 2s 195ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 2s 194ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 2s 277ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 2s 308ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 2s 193ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 2s 191ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 2s 232ms/step - loss: -1902.7975 - val_loss: -1903.9446\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 2s 295ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 2s 191ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 2s 300ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 2s 228ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 2s 191ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 2s 193ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 2s 235ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 2s 290ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 2s 189ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 2s 193ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 2s 194ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 2s 318ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 2s 226ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 2s 245ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 2s 291ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 2s 240ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 2s 239ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 2s 238ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 2s 191ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 2s 265ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 2s 253ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 2s 195ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 2s 194ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 3s 318ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 2s 312ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 2s 217ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 2s 272ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 1s 190ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 2s 281ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 2s 244ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 2s 195ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 2s 194ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 2s 193ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 3s 324ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 2s 192ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 1s 189ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 3s 225ms/step - loss: -130.3214 - val_loss: -1161.4004\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1776.5326 - val_loss: -1903.7086\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 2s 190ms/step - loss: -1902.7131 - val_loss: -1903.9050\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 2s 300ms/step - loss: -1902.7595 - val_loss: -1903.9183\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 2s 240ms/step - loss: -1902.7781 - val_loss: -1903.9324\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7933 - val_loss: -1903.9408\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7971 - val_loss: -1903.9437\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 2s 195ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 2s 238ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 2s 284ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 2s 195ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 2s 206ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 3s 352ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 2s 328ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 2s 214ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 2s 206ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 2s 279ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 2s 327ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 2s 267ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 2s 264ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 2s 231ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 2s 306ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 2s 206ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 2s 211ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 3s 337ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 2s 206ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 3s 348ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 2s 207ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 2s 289ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 2s 245ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 2s 205ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 2s 249ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 2s 295ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 2s 193ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 3s 355ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 2s 207ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 2s 212ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 2s 205ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 3s 358ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 2s 206ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 2s 318ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 2s 222ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 2s 205ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 4s 277ms/step - loss: -220.1031 - val_loss: -1464.2173\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1839.5543 - val_loss: -1902.5950\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.5492 - val_loss: -1903.9368\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7927 - val_loss: -1903.9430\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 2s 191ms/step - loss: -1902.7963 - val_loss: -1903.9437\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 2s 194ms/step - loss: -1902.7972 - val_loss: -1903.9441\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 2s 213ms/step - loss: -1902.7975 - val_loss: -1903.9446\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 2s 303ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 2s 197ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 2s 207ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 3s 361ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 2s 214ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 2s 210ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 2s 322ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 2s 216ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 2s 208ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 2s 207ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 2s 209ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 2s 308ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 2s 232ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 1s 189ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 2s 210ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 2s 213ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 2s 283ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 2s 264ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 2s 211ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 2s 208ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 2s 265ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 2s 266ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 2s 205ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 2s 235ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 3s 317ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 2s 208ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 2s 196ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 2s 206ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 3s 347ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 2s 198ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 3s 338ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 2s 213ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 2s 212ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 2s 211ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 2s 217ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 2s 214ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 3s 344ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 2s 210ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 2s 209ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 2s 209ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 3s 331ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 2s 206ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 2s 199ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 2s 200ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 2s 327ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 2s 224ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 2s 203ms/step - loss: -1902.7976 - val_loss: -1903.9446\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 2s 207ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 2s 209ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 2s 207ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 2s 201ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 2s 293ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 2s 246ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 2s 204ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 2s 208ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 2s 202ms/step - loss: -1902.7979 - val_loss: -1903.9446\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 2s 277ms/step - loss: -1902.7981 - val_loss: -1903.9446\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Lambda value: 25\n",
            "Average PSNR values:\n",
            "Autoencoder: 5.93\n",
            "Median Filter: 31.96\n",
            "Mean Filter: 30.77\n",
            "Gaussian Filter: 32.69\n",
            "Bilateral Filter: 32.94\n",
            "Lambda value: 50\n",
            "Average PSNR values:\n",
            "Autoencoder: 5.93\n",
            "Median Filter: 31.96\n",
            "Mean Filter: 30.77\n",
            "Gaussian Filter: 32.69\n",
            "Bilateral Filter: 32.94\n",
            "Lambda value: 75\n",
            "Average PSNR values:\n",
            "Autoencoder: 5.93\n",
            "Median Filter: 31.96\n",
            "Mean Filter: 30.77\n",
            "Gaussian Filter: 32.69\n",
            "Bilateral Filter: 32.94\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def apply_median_filter(img):\n",
        "    return cv2.medianBlur(img, 3)\n",
        "\n",
        "def apply_mean_filter(img):\n",
        "    return cv2.blur(img, (3, 3))\n",
        "\n",
        "def apply_gaussian_filter(img):\n",
        "    return cv2.GaussianBlur(img, (3, 3), 0)\n",
        "\n",
        "def apply_bilateral_filter(img):\n",
        "    return cv2.bilateralFilter(img, 5, 75, 75)\n",
        "\n",
        "def get_psnr(original, processed):\n",
        "    mse = np.mean((original - processed) ** 2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    max_pixel = 255.0\n",
        "    psnr = 10 * np.log10((max_pixel ** 2) / mse)\n",
        "    return psnr\n",
        "\n",
        "def add_poisson(img, lambda_val):\n",
        "  poisson_noise = np.random.poisson(lambda_val, size=img.shape)\n",
        "  noisy_image = img + poisson_noise\n",
        "  if noisy_image.dtype != np.uint8:\n",
        "    noisy_image = cv2.convertScaleAbs(noisy_image)\n",
        "  return noisy_image\n",
        "\n",
        "# Get the images in the pneumonia folder\n",
        "data_folder = \"/content/pneumonia/\"\n",
        "image_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.jpeg')]\n",
        "images = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in image_files]\n",
        "\n",
        "input_size = (124, 124)\n",
        "images_resized = [cv2.resize(img, input_size) for img in images]\n",
        "images_resized = np.array(images_resized)\n",
        "\n",
        "train_images, test_images = train_test_split(images_resized, test_size=0.2, random_state=42)\n",
        "train_images = tf.convert_to_tensor(train_images, dtype=tf.float32)\n",
        "test_images = tf.convert_to_tensor(test_images, dtype=tf.float32)\n",
        "\n",
        "# Autoencoder\n",
        "def create_autoencoder_residual(input_shape, z):\n",
        "    input_img = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x_skip1 = x  # Skip connection 1\n",
        "\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x_skip2 = x  # Skip connection 2\n",
        "\n",
        "    x = Conv2D(z, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same', name='bottleneck')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = tf.concat([x, x_skip2], axis=-1)  # Skip connection 2\n",
        "\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = tf.concat([x, x_skip1], axis=-1)  # Skip connection 1\n",
        "\n",
        "    x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('bottleneck').output)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    return autoencoder, encoder\n",
        "\n",
        "def create_autoencoder_inception(input_shape, z):\n",
        "    input_img = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = inception(input_img, 16)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = inception(x, 8)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(z, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same', name='bottleneck')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = inception(x, 8)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = inception(x, 16)\n",
        "\n",
        "    x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('bottleneck').output)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    return autoencoder, encoder\n",
        "\n",
        "def encode(image):\n",
        "    return encoder.predict(image)\n",
        "\n",
        "def inception(x, filters):\n",
        "    conv_1x1 = Conv2D(filters // 4, (1, 1), padding='same', activation='relu')(x)\n",
        "    conv_3x3 = Conv2D(filters // 4, (3, 3), padding='same', activation='relu')(x)\n",
        "    conv_5x5 = Conv2D(filters // 4, (5, 5), padding='same', activation='relu')(x)\n",
        "    max_pool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    max_pool = Conv2D(filters // 4, (1, 1), padding='same', activation='relu')(max_pool)\n",
        "\n",
        "    x = tf.concat([conv_1x1, conv_3x3, conv_5x5, max_pool], axis=-1)\n",
        "    return x\n",
        "\n",
        "lambda_values = [25, 50, 75]\n",
        "noisy_images_list = []\n",
        "for lambda_value in lambda_values:\n",
        "    noisy_images = []\n",
        "    for image in images_resized:\n",
        "        noisy_image = add_poisson(image, lambda_value)\n",
        "        noisy_images.append(noisy_image)\n",
        "    noisy_images_list.append(noisy_images)\n",
        "\n",
        "average_psnr_values = {}\n",
        "input_shape = (124, 124, 1)\n",
        "latent_dim = 32\n",
        "for idx, lambda_value in enumerate(lambda_values):\n",
        "    autoencoder, encoder = create_autoencoder_inception(input_shape, latent_dim)  # Create autoencoder with specified input shape and latent dimension\n",
        "    noisy_images = np.array(noisy_images_list[idx])\n",
        "    autoencoder.fit(noisy_images, images_resized, epochs=100, batch_size=10, shuffle=True, validation_split=0.2)\n",
        "\n",
        "    autoencoder.save_weights('autoencoder_weights.h5')\n",
        "\n",
        "    psnr_values_autoencoder = []\n",
        "    psnr_values_median = []\n",
        "    psnr_values_mean = []\n",
        "    psnr_values_gaussian = []\n",
        "    psnr_values_bilateral = []\n",
        "\n",
        "    for test_image in test_images:\n",
        "        denoised_image_autoencoder = autoencoder.predict(test_image[np.newaxis, ...])[0, ...]\n",
        "        psnr_autoencoder = get_psnr(test_image, denoised_image_autoencoder)\n",
        "        psnr_values_autoencoder.append(psnr_autoencoder)\n",
        "\n",
        "        noisy_image = test_image.numpy()\n",
        "        denoised_image_median = apply_median_filter(noisy_image)\n",
        "        denoised_image_mean = apply_mean_filter(noisy_image)\n",
        "        denoised_image_gaussian = apply_gaussian_filter(noisy_image)\n",
        "        denoised_image_bilateral = apply_bilateral_filter(noisy_image)\n",
        "\n",
        "        psnr_median = get_psnr(test_image, denoised_image_median)\n",
        "        psnr_mean = get_psnr(test_image, denoised_image_mean)\n",
        "        psnr_gaussian = get_psnr(test_image, denoised_image_gaussian)\n",
        "        psnr_bilateral = get_psnr(test_image, denoised_image_bilateral)\n",
        "\n",
        "        psnr_values_median.append(psnr_median)\n",
        "        psnr_values_mean.append(psnr_mean)\n",
        "        psnr_values_gaussian.append(psnr_gaussian)\n",
        "        psnr_values_bilateral.append(psnr_bilateral)\n",
        "\n",
        "    average_psnr_values[lambda_value] = {\n",
        "        \"Autoencoder\": np.mean(psnr_values_autoencoder),\n",
        "        \"Median Filter\": np.mean(psnr_values_median),\n",
        "        \"Mean Filter\": np.mean(psnr_values_mean),\n",
        "        \"Gaussian Filter\": np.mean(psnr_values_gaussian),\n",
        "        \"Bilateral Filter\": np.mean(psnr_values_bilateral)\n",
        "    }\n",
        "\n",
        "# Print average PSNR values for each lambda value\n",
        "for lambda_value, psnr_values in average_psnr_values.items():\n",
        "    print(f\"Lambda value: {lambda_value}\")\n",
        "    print(f\"Average PSNR values:\")\n",
        "    for method, average_psnr in psnr_values.items():\n",
        "        print(f\"{method}: {average_psnr:.2f}\")\n",
        "\n",
        "encoded_z_values = []\n",
        "for train_image in train_images:\n",
        "    encoded_image = encode(train_image[np.newaxis, ...])\n",
        "    encoded_z_values.append(encoded_image.flatten())  # Flatten the encoded image to store as a 1D array\n",
        "\n",
        "    # Convert the list of encoded z values into a pandas DataFrame\n",
        "    encoded_df = pd.DataFrame(encoded_z_values)\n",
        "\n",
        "    # Save the data to a csv file\n",
        "    encoded_df.to_csv(f'z_values.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which approach performed better for this particular task of denoising?\n",
        "\n",
        "The residual skip connections had better average PSNR values than inception. This could be because residual skip connections skipping connections, which can preserve important details and structural information.\n",
        "\n",
        "2. What is the intuition behind the improved model which made it ideal for this particular task?\n",
        "The intuition behind residual skip connections is to allow the network to bypass certain layers and distribute the input directly to deeper layers. This can help mitigate the vanishing gradient problem in neural networks and lets the network learn residual representations, which can improve image denoising.\n",
        "\n",
        "The intuition behind inception is to capture different levels of detail and information by processing the input through multiple convolutional filters in parallel, with different kernel sizes. This can help the network learn more diverse and discriminative features, which can also improve image denoising."
      ],
      "metadata": {
        "id": "j01GEJ29IhbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "https://www.researchgate.net/publication/311609041_Deep_Residual_Learning_for_Image_Recognition\n",
        "\n",
        "https://www.researchgate.net/publication/265787949_Going_Deeper_with_Convolutions\n",
        "\n"
      ],
      "metadata": {
        "id": "2Yh89JwJXyuV"
      }
    }
  ]
}